Warning: Detected a function call instruction:
Warning: Ignoring called function instructions
Warning: Detected a function call instruction:
Warning: Ignoring called function instructions
Warning: Detected a function call instruction:
Warning: Ignoring called function instructions
Warning: Detected a function call instruction:
Warning: Ignoring called function instructions
Warning: Detected a function call instruction:
Warning: Ignoring called function instructions
Warning: Detected a function call instruction:
Warning: Ignoring called function instructions
Warning: Detected a function call instruction:
Warning: Ignoring called function instructions
Warning: Detected a function call instruction:
Warning: Ignoring called function instructions
Warning: Detected a function call instruction:
Warning: Ignoring called function instructions
Warning: Detected a function call instruction:
Warning: Ignoring called function instructions
Warning: Detected a function call instruction:
Warning: Ignoring called function instructions
Info: Assuming lines 195 and 200 correspond to the same source loop
Section 1: Function: main
=========================

These loops are supposed to be defined in: /home/zj58/cs5220/OpenMP/1/sim.c

Section 1.1: Source loop ending at line 53
==========================================

Composition and unrolling
-------------------------
It is composed of the following loops [ID (first-last source line)]:
 - 17 (51-53)
 - 18 (51-53)
 - 19 (51-53)
 - 20 (51-53)
 - 27 (51-53)
 - 28 (51-53)
 - 29 (51-53)
 - 30 (51-53)
and is unrolled by 32 (including vectorization).

The following loops are considered as:
 - unrolled and/or vectorized: 19, 29
 - peel or tail: 17, 20, 27, 30
The analysis will be displayed for the unrolled and/or vectorized loops: 19, 29

Section 1.1.1: Binary (unrolled and/or vectorized) loop #19
===========================================================

Type of elements and instruction set
------------------------------------
16 AVX instructions are processing arithmetic or math operations on double precision FP elements in vector mode (four at a time).

Vectorization
-------------
Your loop is fully vectorized (all SSE/AVX instructions are used in vector mode and on full vector length).

Matching between your loop (in the source code) and the binary loop
-------------------------------------------------------------------
The binary loop is composed of 96 FP arithmetical operations:
 - 32: addition or subtraction
 - 32: fused multiply-add
The binary loop is loading 512 bytes (64 double precision FP elements).

Arithmetic intensity is 0.19 FP operations per loaded or stored byte.

Cycles and resources usage
--------------------------
Assuming all data fit into the L1 cache, each iteration of the binary loop takes 8.50 cycles. At this rate:
 - 70% of peak computational performance is reached (11.29 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))
 - 94% of peak load performance is reached (60.24 out of 64.00 bytes loaded per cycle (GB/s @ 1GHz))

Pathological cases
------------------
None detected.

Bottlenecks
-----------
The ROB-read stage is a bottleneck.
By removing all these bottlenecks, you can lower the cost of an iteration from 8.50 to 8.00 cycles (1.06x speedup).


Section 1.1.2: Binary (unrolled and/or vectorized) loop #29
===========================================================

Type of elements and instruction set
------------------------------------
16 AVX instructions are processing arithmetic or math operations on double precision FP elements in vector mode (four at a time).

Vectorization
-------------
Your loop is fully vectorized (all SSE/AVX instructions are used in vector mode and on full vector length).

Matching between your loop (in the source code) and the binary loop
-------------------------------------------------------------------
The binary loop is composed of 96 FP arithmetical operations:
 - 32: addition or subtraction
 - 32: fused multiply-add
The binary loop is loading 512 bytes (64 double precision FP elements).

Arithmetic intensity is 0.19 FP operations per loaded or stored byte.

Cycles and resources usage
--------------------------
Assuming all data fit into the L1 cache, each iteration of the binary loop takes 8.50 cycles. At this rate:
 - 70% of peak computational performance is reached (11.29 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))
 - 94% of peak load performance is reached (60.24 out of 64.00 bytes loaded per cycle (GB/s @ 1GHz))

Pathological cases
------------------
None detected.

Bottlenecks
-----------
The ROB-read stage is a bottleneck.
By removing all these bottlenecks, you can lower the cost of an iteration from 8.50 to 8.00 cycles (1.06x speedup).

Section 1.2: Source loop ending at line 72
==========================================

Composition and unrolling
-------------------------
It is composed of the following loops [ID (first-last source line)]:
 - 12 (71-72)
 - 13 (71-72)
 - 14 (71-72)
 - 22 (71-72)
 - 23 (71-72)
 - 24 (71-72)
and is unrolled by 8 (including vectorization).

The following loops are considered as:
 - unrolled and/or vectorized: 14, 24
 - peel or tail: 12, 22
The analysis will be displayed for the unrolled and/or vectorized loops: 14, 24

Section 1.2.1: Binary (unrolled and/or vectorized) loop #14
===========================================================

Type of elements and instruction set
------------------------------------
12 SSE or AVX instructions are processing arithmetic or math operations on double precision FP elements in vector mode (two at a time).

Vectorization
-------------
Your loop is vectorized (all SSE/AVX instructions are used in vector mode) but on 50% vector length.

Matching between your loop (in the source code) and the binary loop
-------------------------------------------------------------------
The binary loop is composed of 32 FP arithmetical operations:
 - 16: addition or subtraction
 - 8: fused multiply-add
The binary loop is loading 128 bytes (16 double precision FP elements).
The binary loop is storing 64 bytes (8 double precision FP elements).

Arithmetic intensity is 0.17 FP operations per loaded or stored byte.

Cycles and resources usage
--------------------------
Assuming all data fit into the L1 cache, each iteration of the binary loop takes 8.00 cycles. At this rate:
 - 25% of peak computational performance is reached (4.00 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))
 - 25% of peak load performance is reached (16.00 out of 64.00 bytes loaded per cycle (GB/s @ 1GHz))
 - 25% of peak store performance is reached (8.00 out of 32.00 bytes stored per cycle (GB/s @ 1GHz))

Pathological cases
------------------
Your loop is processing FP elements but is NOT OR PARTIALLY VECTORIZED and could benefit from full vectorization.
Since your execution units are vector units, only a fully vectorized loop can use their full power.
By fully vectorizing your loop, you can lower the cost of an iteration from 8.00 to 4.00 cycles (2.00x speedup).
Propositions:
 - Pass to your compiler a micro-architecture specialization option:
  * Intel: use axHost or xHost.
 - Use vector aligned instructions:
  1) align your arrays on 32 bytes boundaries,
  2) inform your compiler that your arrays are vector aligned:
   * Intel: use the VECTOR ALIGNED directive.
 - Use the LOOP COUNT directive

Fix as many pathological cases as you can before reading the following sections.

Bottlenecks
-----------
The FP add unit is a bottleneck.
Try to reduce the number of FP add instructions.

By removing all these bottlenecks, you can lower the cost of an iteration from 8.00 to 7.50 cycles (1.07x speedup).


Section 1.2.2: Binary (unrolled and/or vectorized) loop #24
===========================================================

Type of elements and instruction set
------------------------------------
12 SSE or AVX instructions are processing arithmetic or math operations on double precision FP elements in vector mode (two at a time).

Vectorization
-------------
Your loop is vectorized (all SSE/AVX instructions are used in vector mode) but on 50% vector length.

Matching between your loop (in the source code) and the binary loop
-------------------------------------------------------------------
The binary loop is composed of 32 FP arithmetical operations:
 - 16: addition or subtraction
 - 8: fused multiply-add
The binary loop is loading 128 bytes (16 double precision FP elements).
The binary loop is storing 64 bytes (8 double precision FP elements).

Arithmetic intensity is 0.17 FP operations per loaded or stored byte.

Cycles and resources usage
--------------------------
Assuming all data fit into the L1 cache, each iteration of the binary loop takes 8.00 cycles. At this rate:
 - 25% of peak computational performance is reached (4.00 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))
 - 25% of peak load performance is reached (16.00 out of 64.00 bytes loaded per cycle (GB/s @ 1GHz))
 - 25% of peak store performance is reached (8.00 out of 32.00 bytes stored per cycle (GB/s @ 1GHz))

Pathological cases
------------------
Your loop is processing FP elements but is NOT OR PARTIALLY VECTORIZED and could benefit from full vectorization.
Since your execution units are vector units, only a fully vectorized loop can use their full power.
By fully vectorizing your loop, you can lower the cost of an iteration from 8.00 to 4.00 cycles (2.00x speedup).
Propositions:
 - Pass to your compiler a micro-architecture specialization option:
  * Intel: use axHost or xHost.
 - Use vector aligned instructions:
  1) align your arrays on 32 bytes boundaries,
  2) inform your compiler that your arrays are vector aligned:
   * Intel: use the VECTOR ALIGNED directive.
 - Use the LOOP COUNT directive

Fix as many pathological cases as you can before reading the following sections.

Bottlenecks
-----------
The FP add unit is a bottleneck.
Try to reduce the number of FP add instructions.

By removing all these bottlenecks, you can lower the cost of an iteration from 8.00 to 6.75 cycles (1.19x speedup).

Section 1.3: Source loop ending at line 90
==========================================

Composition and unrolling
-------------------------
It is composed of the following loops [ID (first-last source line)]:
 - 8 (88-90)
 - 9 (88-90)
 - 10 (88-90)
and is unrolled by 32 (including vectorization).

The following loops are considered as:
 - unrolled and/or vectorized: 10
 - peel or tail: 8
The analysis will be displayed for the unrolled and/or vectorized loops: 10

Section 1.3.1: Binary (unrolled and/or vectorized) loop #10
===========================================================

Type of elements and instruction set
------------------------------------
8 AVX instructions are processing arithmetic or math operations on double precision FP elements in vector mode (four at a time).

Vectorization
-------------
Your loop is fully vectorized (all SSE/AVX instructions are used in vector mode and on full vector length).

Matching between your loop (in the source code) and the binary loop
-------------------------------------------------------------------
The binary loop is composed of 64 FP arithmetical operations:
 - 32: fused multiply-add
The binary loop is loading 512 bytes (64 double precision FP elements).

Arithmetic intensity is 0.12 FP operations per loaded or stored byte.

Cycles and resources usage
--------------------------
Assuming all data fit into the L1 cache, each iteration of the binary loop takes 8.00 cycles. At this rate:
 - 50% of peak computational performance is reached (8.00 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))
 - 100% of peak load performance is reached (64.00 out of 64.00 bytes loaded per cycle (GB/s @ 1GHz))

Pathological cases
------------------
None detected.

Bottlenecks
-----------
Load units are a bottleneck.
Try to reduce the number of loads.
For example, provide more information to your compiler:
 - hardcode the bounds of the corresponding 'for' loop,
 -  C/C++: use the 'restrict' C99 keyword
 -  Intel: you can also use the fno-alias option

By removing all these bottlenecks, you can lower the cost of an iteration from 8.00 to 6.50 cycles (1.23x speedup).

Section 1.4: Source loop ending at line 107
===========================================

Composition and unrolling
-------------------------
It is composed of the following loops [ID (first-last source line)]:
 - 3 (105-107)
 - 4 (105-107)
 - 5 (105-107)
and is unrolled by 8 (including vectorization).

The following loops are considered as:
 - unrolled and/or vectorized: 5
 - peel or tail: 3
The analysis will be displayed for the unrolled and/or vectorized loops: 5

Section 1.4.1: Binary (unrolled and/or vectorized) loop #5
==========================================================

Type of elements and instruction set
------------------------------------
4 SSE or AVX instructions are processing arithmetic or math operations on double precision FP elements in vector mode (two at a time).

Vectorization
-------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar mode and, for others, at least one is in vector mode).
Only 21% of vector length is used.

Matching between your loop (in the source code) and the binary loop
-------------------------------------------------------------------
The binary loop is composed of 16 FP arithmetical operations:
 - 8: fused multiply-add
The binary loop is loading 160 bytes (20 double precision FP elements).

Arithmetic intensity is 0.10 FP operations per loaded or stored byte.

Cycles and resources usage
--------------------------
Assuming all data fit into the L1 cache, each iteration of the binary loop takes 7.50 cycles. At this rate:
 - 13% of peak computational performance is reached (2.13 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))
 - 33% of peak load performance is reached (21.33 out of 64.00 bytes loaded per cycle (GB/s @ 1GHz))

Pathological cases
------------------
Your loop is processing FP elements but is NOT OR PARTIALLY VECTORIZED and could benefit from full vectorization.
Since your execution units are vector units, only a fully vectorized loop can use their full power.
By fully vectorizing your loop, you can lower the cost of an iteration from 7.50 to 3.27 cycles (2.29x speedup).
Two propositions:
 - Try another compiler or update/tune your current one:
  * Intel: use the vec-report option to understand why your loop was not vectorized. If "existence of vector dependences", try the IVDEP directive. If, using IVDEP, "vectorization possible but seems inefficient", try the VECTOR ALWAYS directive.
 - Remove inter-iterations dependences from your loop and make it unit-stride.

Fix as many pathological cases as you can before reading the following sections.

Bottlenecks
-----------
The ROB-read stage is a bottleneck.
By removing all these bottlenecks, you can lower the cost of an iteration from 7.50 to 6.00 cycles (1.25x speedup).

Section 1.5: Source loop ending at line 161
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 36
and is not unrolled or unrolled with no peel/tail code (including vectorization).
The analysis will be displayed for the first found loop: 36

Type of elements and instruction set
------------------------------------

Vectorization
-------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar mode).
Only 18% of vector length is used.

Matching between your loop (in the source code) and the binary loop
-------------------------------------------------------------------
The binary loop does not contain any FP arithmetical operations.
The binary loop is storing 16 bytes.


Cycles and resources usage
--------------------------
Assuming all data fit into the L1 cache, each iteration of the binary loop takes 2.00 cycles. At this rate:
 - 25% of peak store performance is reached (8.00 out of 32.00 bytes stored per cycle (GB/s @ 1GHz))

Pathological cases
------------------
Performance is bounded by DATA DEPENDENCIES.
By removing most critical dependency chains, you can lower the cost of an iteration from 2.00 to 0.75 cycles (2.67x speedup).
Two propositions:
 - Try another compiler or update/tune your current one:
  * Intel: use the vec-report option. If "existence of vector dependences", try the IVDEP directive.
 - Remove inter-iterations dependences from your loop.

Fix as many pathological cases as you can before reading the following sections.

Bottlenecks
-----------
By removing all these bottlenecks, you can lower the cost of an iteration from 2.00 to 0.75 cycles (2.67x speedup).

Section 1.6: Source loop ending at line 188
===========================================

Composition and unrolling
-------------------------
It is composed of the following loops [ID (first-last source line)]:
 - 33 (187-188)
 - 34 (187-188)
and is unrolled by 8 (including vectorization).

The following loops are considered as:
 - unrolled and/or vectorized: 34
 - peel or tail: 33
The analysis will be displayed for the unrolled and/or vectorized loops: 34

Section 1.6.1: Binary (unrolled and/or vectorized) loop #34
===========================================================

Type of elements and instruction set
------------------------------------

Vectorization
-------------
Your loop is fully vectorized (all SSE/AVX instructions are used in vector mode and on full vector length).

Matching between your loop (in the source code) and the binary loop
-------------------------------------------------------------------
The binary loop does not contain any FP arithmetical operations.
The binary loop is storing 32 bytes.


Cycles and resources usage
--------------------------
Assuming all data fit into the L1 cache, each iteration of the binary loop takes 2.00 cycles. At this rate:
 - 50% of peak store performance is reached (16.00 out of 32.00 bytes stored per cycle (GB/s @ 1GHz))

Pathological cases
------------------
Performance is bounded by DATA DEPENDENCIES.
By removing most critical dependency chains, you can lower the cost of an iteration from 2.00 to 0.75 cycles (2.67x speedup).
Two propositions:
 - Try another compiler or update/tune your current one:
  * Intel: use the vec-report option. If "existence of vector dependences", try the IVDEP directive.
 - Remove inter-iterations dependences from your loop.

Fix as many pathological cases as you can before reading the following sections.

Bottlenecks
-----------
By removing all these bottlenecks, you can lower the cost of an iteration from 2.00 to 0.75 cycles (2.67x speedup).

Section 1.7: Source loop ending at line 195
===========================================

Composition and unrolling
-------------------------
It is composed of the following loops [ID (first-last source line)]:
 - 31 (15-200)
 - 32 (15-195)
and is multi-versionned but not unrolled (including vectorization).
The analysis will be displayed for the first found loop: 31

Type of elements and instruction set
------------------------------------
4 SSE or AVX instructions are processing arithmetic or math operations on double precision FP elements in scalar mode (one at a time).

Vectorization
-------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar mode and, for others, at least one is in vector mode).
Only 26% of vector length is used.

Matching between your loop (in the source code) and the binary loop
-------------------------------------------------------------------
The binary loop is composed of 4 FP arithmetical operations:
 - 3: multiply
 - 1: square root
The binary loop is loading 80 bytes (10 double precision FP elements).
The binary loop is storing 32 bytes (4 double precision FP elements).

Arithmetic intensity is 0.04 FP operations per loaded or stored byte.

Cycles and resources usage
--------------------------
Assuming all data fit into the L1 cache, each iteration of the binary loop takes 14.00 cycles. At this rate:
 - 1% of peak computational performance is reached (0.29 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))
 - 8% of peak load performance is reached (5.71 out of 64.00 bytes loaded per cycle (GB/s @ 1GHz))
 - 7% of peak store performance is reached (2.29 out of 32.00 bytes stored per cycle (GB/s @ 1GHz))

Pathological cases
------------------
Your loop is processing FP elements but is NOT OR PARTIALLY VECTORIZED and could benefit from full vectorization.
Since your execution units are vector units, only a fully vectorized loop can use their full power.
By fully vectorizing your loop, you can lower the cost of an iteration from 14.00 to 7.00 cycles (2.00x speedup).
Two propositions:
 - Try another compiler or update/tune your current one:
  * Intel: use the vec-report option to understand why your loop was not vectorized. If "existence of vector dependences", try the IVDEP directive. If, using IVDEP, "vectorization possible but seems inefficient", try the VECTOR ALWAYS directive.
 - Remove inter-iterations dependences from your loop and make it unit-stride.

Fix as many pathological cases as you can before reading the following sections.

Bottlenecks
-----------
The divide/square root unit is a bottleneck.
Try to reduce the number of division or square root instructions.
Check whether you really need double precision. If not, switch to single precision to speedup execution.

By removing all these bottlenecks, you can lower the cost of an iteration from 14.00 to 4.00 cycles (3.50x speedup).

Section 1.8: Source loop ending at line 270
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 37
and is not unrolled or unrolled with no peel/tail code (including vectorization).
The analysis will be displayed for the first found loop: 37

Type of elements and instruction set
------------------------------------

Vectorization
-------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar mode).
Only 25% of vector length is used.

Matching between your loop (in the source code) and the binary loop
-------------------------------------------------------------------
The binary loop does not contain any FP arithmetical operations.
The binary loop is loading 8 bytes.


Cycles and resources usage
--------------------------
Assuming all data fit into the L1 cache, each iteration of the binary loop takes 0.25 cycles. At this rate:
 - 50% of peak load performance is reached (32.00 out of 64.00 bytes loaded per cycle (GB/s @ 1GHz))

Pathological cases
------------------
None detected.

Bottlenecks
-----------
Front-end is a bottleneck.
The ROB-read stage is a bottleneck.
The P0 port or a related execution unit (except SQRT/DIV and FP multiply) is a bottleneck.

The P1 port or a related execution unit (except FP add) is a bottleneck.

The P5 port or a related execution unit is a bottleneck.

By removing all these bottlenecks, you can lower the cost of an iteration from 0.25 to 0.00 cycles (infx speedup).

Section 1.9: Source loop ending at line 278
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 38
and is not unrolled or unrolled with no peel/tail code (including vectorization).
The analysis will be displayed for the first found loop: 38

Type of elements and instruction set
------------------------------------

Vectorization
-------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar mode).
Only 25% of vector length is used.

Matching between your loop (in the source code) and the binary loop
-------------------------------------------------------------------
The binary loop does not contain any FP arithmetical operations.
The binary loop is loading 8 bytes.


Cycles and resources usage
--------------------------
Assuming all data fit into the L1 cache, each iteration of the binary loop takes 0.25 cycles. At this rate:
 - 50% of peak load performance is reached (32.00 out of 64.00 bytes loaded per cycle (GB/s @ 1GHz))

Pathological cases
------------------
None detected.

Bottlenecks
-----------
Front-end is a bottleneck.
The ROB-read stage is a bottleneck.
The P0 port or a related execution unit (except SQRT/DIV and FP multiply) is a bottleneck.

The P1 port or a related execution unit (except FP add) is a bottleneck.

The P5 port or a related execution unit is a bottleneck.

By removing all these bottlenecks, you can lower the cost of an iteration from 0.25 to 0.00 cycles (infx speedup).

Section 1.10: Source loop ending at line 517
============================================

Composition and unrolling
-------------------------
It is composed of the loop 6
and is not unrolled or unrolled with no peel/tail code (including vectorization).
The analysis will be displayed for the first found loop: 6

Type of elements and instruction set
------------------------------------
1 SSE or AVX instructions are processing arithmetic or math operations on double precision FP elements in scalar mode (one at a time).

Vectorization
-------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar mode and, for others, at least one is in vector mode).
Only 22% of vector length is used.

Matching between your loop (in the source code) and the binary loop
-------------------------------------------------------------------
The binary loop is composed of 2 FP arithmetical operations:
 - 1: fused multiply-add
The binary loop is loading 28 bytes (3 double precision FP elements).
The binary loop is storing 8 bytes (1 double precision FP elements).

Arithmetic intensity is 0.06 FP operations per loaded or stored byte.

Cycles and resources usage
--------------------------
Assuming all data fit into the L1 cache, each iteration of the binary loop takes 4.00 cycles. At this rate:
 - 3% of peak computational performance is reached (0.50 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))
 - 10% of peak load performance is reached (7.00 out of 64.00 bytes loaded per cycle (GB/s @ 1GHz))
 - 6% of peak store performance is reached (2.00 out of 32.00 bytes stored per cycle (GB/s @ 1GHz))

Pathological cases
------------------
Performance is bounded by DATA DEPENDENCIES.
By removing most critical dependency chains, you can lower the cost of an iteration from 4.00 to 2.25 cycles (1.78x speedup).
Two propositions:
 - Try another compiler or update/tune your current one:
  * Intel: use the vec-report option. If "existence of vector dependences", try the IVDEP directive.
 - Remove inter-iterations dependences from your loop.

Detected EXPENSIVE INSTRUCTIONS, generating more than one micro-operation.
Only one of these instructions can be decoded during a cycle and the extra micro-operations increase pressure on execution units.
VCVTTSD2SI: 1 occurrences
 - Pass to your compiler a micro-architecture specialization option:
  * Intel: use axHost or xHost.

Fix as many pathological cases as you can before reading the following sections.

Bottlenecks
-----------
By removing all these bottlenecks, you can lower the cost of an iteration from 4.00 to 2.25 cycles (1.78x speedup).


Loops with the following IDs cannot be analyzed: 35
